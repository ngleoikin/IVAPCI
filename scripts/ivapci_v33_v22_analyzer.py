"""IVAPCI v3.3 (version 22) result analyzer.

Reads benchmark/diagnostic CSVs and prints summary + quality/overlap analyses.
Defaults match the filenames mentioned in user feedback; override via CLI flags.
"""

from __future__ import annotations

import argparse
import warnings
from typing import Dict, List

import numpy as np
import pandas as pd
import seaborn as sns
from scipy import stats


warnings.filterwarnings("ignore")
sns.set_style("whitegrid")


class MethodComparison:
    """æ–¹æ³•çº§æ€§èƒ½å¯¹æ¯”ä¸æ˜¾è‘—æ€§åˆ†æã€‚"""

    def __init__(self, df_bench: pd.DataFrame, df_summary: pd.DataFrame):
        self.df_bench = df_bench
        self.df_summary = df_summary

    def comprehensive_method_comparison(self) -> None:
        """æŒ‰æ–¹æ³•èšåˆè¾“å‡ºï¼šæ ¸å¿ƒæŒ‡æ ‡ã€æ˜¾è‘—æ€§ã€é²æ£’æ€§ã€è¡¨å¾åˆ†æä¸æ•ˆç‡ã€‚"""

        print("\n" + "=" * 80)
        print(" " * 20 + "ğŸ“Š æ–¹æ³•æ€§èƒ½ç»¼åˆå¯¹æ¯”")
        print("=" * 80)

        methods = list(self.df_summary["method"].unique()) if "method" in self.df_summary.columns else []
        if not methods:
            print("  â„¹ï¸  æœªæ‰¾åˆ°æ–¹æ³•åˆ—ï¼Œè·³è¿‡æ–¹æ³•å¯¹æ¯”")
            return

        self._print_performance_table(methods)
        self._print_significance_tests(methods)
        self._print_scenario_robustness(methods)
        self._print_representation_methods_analysis()
        self._print_efficiency_tradeoff(methods)

    def _print_performance_table(self, methods: List[str]) -> None:
        print("\nã€1ï¸âƒ£ æ ¸å¿ƒæ€§èƒ½æŒ‡æ ‡å¯¹æ¯”ã€‘")
        print("  æ–¹æ³•åç§°".ljust(25) + "RMSE    MAE     åå·®    CI95    æ—¶é—´(s)  æ’å")
        print("  " + "-" * 75)

        stats_list: List[Dict[str, float]] = []
        for method in methods:
            df_m = self.df_summary[self.df_summary["method"] == method]
            df_bench_m = self.df_bench[self.df_bench["method"] == method]

            rmse = df_m["rmse"].mean()
            mae = df_m["mean_abs_err"].mean()

            errors = self._signed_errors(df_bench_m)
            bias = float(np.mean(errors)) if errors.size else float("nan")

            if errors.size > 1:
                ci_lower, ci_upper = stats.t.interval(
                    0.95, len(errors) - 1, loc=float(np.mean(errors)), scale=float(stats.sem(errors))
                )
                ci_width = float(ci_upper - ci_lower)
            else:
                ci_width = float("nan")

            runtime = df_m["mean_runtime"].mean()
            stats_list.append(
                {
                    "method": method,
                    "rmse": rmse,
                    "mae": mae,
                    "bias": abs(bias),
                    "ci_width": ci_width,
                    "runtime": runtime,
                }
            )

        stats_df = pd.DataFrame(stats_list).sort_values("rmse")
        for rank, (_, row) in enumerate(stats_df.iterrows(), 1):
            medal = "ğŸ¥‡" if rank == 1 else "ğŸ¥ˆ" if rank == 2 else "ğŸ¥‰" if rank == 3 else "  "
            print(
                f"  {row['method'][:23].ljust(25)}"
                f"{row['rmse']:6.3f}  {row['mae']:6.3f}  {row['bias']:6.3f}  "
                f"{row['ci_width']:6.3f}  {row['runtime']:7.2f}  {medal}{rank}"
            )

    def _print_significance_tests(self, methods: List[str]) -> None:
        print("\nã€2ï¸âƒ£ é…å¯¹tæ£€éªŒï¼ˆvs æœ€ä½³æ–¹æ³•ï¼‰ã€‘")
        if not methods:
            print("  â„¹ï¸ æ–¹æ³•åˆ—è¡¨ä¸ºç©ºï¼Œè·³è¿‡æ˜¾è‘—æ€§æ£€éªŒ")
            return

        best_method = self.df_summary.groupby("method")["rmse"].mean().idxmin()
        best_errors = self._signed_errors(self.df_bench[self.df_bench["method"] == best_method])

        print(f"  åŸºå‡†æ–¹æ³•: {best_method}")
        print("  å¯¹æ¯”æ–¹æ³•".ljust(25) + "å¹³å‡å·®å¼‚  tç»Ÿè®¡é‡  på€¼      æ˜¾è‘—æ€§")
        print("  " + "-" * 65)

        for method in methods:
            if method == best_method:
                continue
            method_errors = self._signed_errors(self.df_bench[self.df_bench["method"] == method])
            if method_errors.size == 0 or method_errors.size != best_errors.size:
                print(f"  {method[:23].ljust(25)}æ•°æ®é‡ä¸åŒ¹é…ï¼Œè·³è¿‡")
                continue
            t_stat, p_val = stats.ttest_rel(np.abs(method_errors), np.abs(best_errors))
            mean_diff = float(np.abs(method_errors).mean() - np.abs(best_errors).mean())
            sig = "***" if p_val < 0.001 else "**" if p_val < 0.01 else "*" if p_val < 0.05 else "n.s."
            print(
                f"  {method[:23].ljust(25)}{mean_diff:+8.4f}  {t_stat:9.3f}  {p_val:7.4f}  {sig}"
            )

    def _print_scenario_robustness(self, methods: List[str]) -> None:
        if "scenario" not in self.df_bench.columns:
            return

        print("\nã€3ï¸âƒ£ åœºæ™¯é²æ£’æ€§åˆ†æã€‘")
        scenarios = self.df_bench["scenario"].unique()

        for method in methods:
            df_m = self.df_bench[self.df_bench["method"] == method]
            scenario_rmses = []
            for scenario in scenarios:
                df_s = df_m[df_m["scenario"] == scenario]
                if len(df_s) > 0:
                    scenario_rmses.append(np.sqrt(df_s["sq_err"].mean()))

            if not scenario_rmses:
                continue

            rmse_std = float(np.std(scenario_rmses))
            best_idx = int(np.argmin(scenario_rmses))
            worst_idx = int(np.argmax(scenario_rmses))
            best_scenario = scenarios[best_idx]
            worst_scenario = scenarios[worst_idx]

            print(f"\n  {method}:")
            print(f"    è·¨åœºæ™¯RMSEæ ‡å‡†å·®: {rmse_std:.4f} {'âœ“ ç¨³å®š' if rmse_std < 0.5 else 'âš ï¸ æ³¢åŠ¨å¤§'}")
            print(f"    æœ€ä½³åœºæ™¯: {best_scenario[:40]}")
            print(f"    æœ€å·®åœºæ™¯: {worst_scenario[:40]}")

    def _print_representation_methods_analysis(self) -> None:
        print("\nã€4ï¸âƒ£ è¡¨å¾å­¦ä¹ æ–¹æ³•ä¸“é¡¹åˆ†æã€‘")

        rep_methods: List[str] = []
        for method in self.df_bench["method"].unique():
            df_m = self.df_bench[self.df_bench["method"] == method]
            if "rep_auc_z_to_a" in df_m.columns and df_m["rep_auc_z_to_a"].notna().any():
                rep_methods.append(method)

        if not rep_methods:
            print("  â„¹ï¸ æ— è¡¨å¾å­¦ä¹ æ–¹æ³•")
            return

        print("  æ–¹æ³•".ljust(25) + "Zâ†’A   Wâ†’A   Zâ†’Yæ³„éœ²  [X,W,A]â†’Y  è´¨é‡åˆ†")
        print("  " + "-" * 70)

        for method in rep_methods:
            df_m = self.df_bench[self.df_bench["method"] == method]
            z_auc = df_m["rep_auc_z_to_a"].mean()
            w_auc = df_m["rep_auc_w_to_a"].mean()
            z_leak = df_m["rep_exclusion_leakage_r2"].mean()
            y_r2 = df_m["rep_r2_xw_a_to_y"].mean()
            qual = df_m["quality_score"].mean()

            z_mark = "âœ“" if z_auc > 0.7 else "âš ï¸" if z_auc > 0.6 else "âœ—"
            w_mark = "âœ“" if 0.45 < w_auc < 0.55 else "âš ï¸"
            leak_mark = "âœ“" if z_leak < 0.1 else "âš ï¸" if z_leak < 0.2 else "âœ—"
            y_mark = "âœ“" if y_r2 > 0.3 else "âœ—"

            print(
                f"  {method[:23].ljust(25)}{z_auc:.2f}{z_mark}  {w_auc:.2f}{w_mark}  "
                f"{z_leak:.2f}{leak_mark}     {y_r2:.2f}{y_mark}      {qual:.1f}/4"
            )

        print("\n  å›¾ä¾‹: Zâ†’A(IVå¼ºåº¦åº”>0.7) | Wâ†’A(ç‹¬ç«‹æ€§åº”â‰ˆ0.5) | Zâ†’Yæ³„éœ²(åº”<0.1) | [X,W,A]â†’Y(åº”>0.3)")

    def _print_efficiency_tradeoff(self, methods: List[str]) -> None:
        print("\nã€5ï¸âƒ£ æ•ˆç‡-ç²¾åº¦æƒè¡¡ã€‘")

        data: List[Dict[str, float]] = []
        for method in methods:
            df_m = self.df_summary[self.df_summary["method"] == method]
            data.append({"method": method, "rmse": df_m["rmse"].mean(), "runtime": df_m["mean_runtime"].mean()})

        df_eff = pd.DataFrame(data)
        if df_eff.empty or df_eff["rmse"].nunique() == 1 or df_eff["runtime"].nunique() == 1:
            print("  â„¹ï¸ æ•°æ®ä¸è¶³ä»¥è®¡ç®—æ•ˆç‡å¾—åˆ†")
            return

        df_eff["rmse_norm"] = (df_eff["rmse"] - df_eff["rmse"].min()) / (df_eff["rmse"].max() - df_eff["rmse"].min())
        df_eff["runtime_norm"] = (df_eff["runtime"] - df_eff["runtime"].min()) / (
            df_eff["runtime"].max() - df_eff["runtime"].min()
        )
        df_eff["efficiency_score"] = (1 - df_eff["rmse_norm"]) - 0.3 * df_eff["runtime_norm"]
        df_eff = df_eff.sort_values("efficiency_score", ascending=False)

        print("  æ–¹æ³•".ljust(25) + "RMSE    è¿è¡Œæ—¶é—´  æ•ˆç‡å¾—åˆ†  æ¨èåœºæ™¯")
        print("  " + "-" * 75)
        for _, row in df_eff.iterrows():
            if row["runtime"] < 0.1:
                scenario = "å®æ—¶æ¨ç†"
            elif row["runtime"] < 5:
                scenario = "åœ¨çº¿å­¦ä¹ "
            else:
                scenario = "ç¦»çº¿è®­ç»ƒ"
            print(
                f"  {row['method'][:23].ljust(25)}{row['rmse']:6.3f}  {row['runtime']:8.2f}s  "
                f"{row['efficiency_score']:8.3f}  {scenario}"
            )

    @staticmethod
    def _signed_errors(df: pd.DataFrame) -> np.ndarray:
        """Return signed ATE errors from available columns.

        Benchmark CSVs expose ``ate_hat`` and ``tau_true`` (plus ``abs_err``/``sq_err``)
        but may lack a precomputed ``err`` column. This helper derives signed errors
        when possible and returns an empty array otherwise.
        """

        if df.empty:
            return np.array([], dtype=float)

        cols = set(df.columns)
        if {"ate_hat", "tau_true"} <= cols:
            return (df["ate_hat"] - df["tau_true"]).to_numpy(dtype=float)
        if "err" in cols:
            return df["err"].to_numpy(dtype=float)
        return np.array([], dtype=float)


class IVAPCIv22Analyzer:
    """Analyze IVAPCI v3.3 version-22 experiment results."""

    def __init__(
        self,
        benchmark_file: str = "simulation_benchmark_results 22.csv",
        diagnostics_file: str = "simulation_diagnostics_results 22.csv",
        summary_file: str = "simulation_benchmark_summary 22.csv",
    ):
        print("ğŸ“‚ åŠ è½½æ•°æ®...")
        self.df_bench = pd.read_csv(benchmark_file)
        self.df_diag = pd.read_csv(diagnostics_file)
        self.df_summary = pd.read_csv(summary_file)

        print(f"   âœ“ Benchmark: {len(self.df_bench)} è¡Œ")
        print(f"   âœ“ Diagnostics: {len(self.df_diag)} è¡Œ")
        print(f"   âœ“ Summary: {len(self.df_summary)} è¡Œ")

        self._compute_quality_scores()

    def _get(self, df: pd.DataFrame, key: str, default: float = np.nan) -> pd.Series:
        return df[key] if key in df.columns else pd.Series([default] * len(df))

    def _compute_quality_scores(self) -> None:
        """Compute a simple 0â€“4 quality score per row."""

        def score_row(row: pd.Series) -> int:
            score = 0
            if row.get("rep_auc_z_to_a", 0) > 0.7:
                score += 1
            w_auc = row.get("rep_auc_w_to_a", 0.5)
            if 0.45 < w_auc < 0.55:
                score += 1
            if row.get("rep_exclusion_leakage_r2", 1.0) < 0.1:
                score += 1
            if row.get("rep_r2_xw_a_to_y", 0) > 0.3:
                score += 1
            return score

        self.df_bench["quality_score"] = self.df_bench.apply(score_row, axis=1)
        self.df_diag["quality_score"] = self.df_diag.apply(score_row, axis=1)

    # ---------------- executive summary ----------------
    def executive_summary(self) -> None:
        print("\n" + "=" * 80)
        print(" " * 20 + "EXECUTIVE SUMMARY")
        print("=" * 80)

        print("\nã€æ•´ä½“æ€§èƒ½ã€‘")
        print(f"  å¹³å‡ç»å¯¹è¯¯å·®: {self.df_bench['abs_err'].mean():.4f}")
        print(f"  RMSE: {np.sqrt(self.df_bench['sq_err'].mean()):.4f}")
        print(f"  å¹³å‡è¿è¡Œæ—¶é—´: {self.df_bench['runtime_sec'].mean():.2f}ç§’")

        if "method" in self.df_summary.columns:
            print("\nã€æ–¹æ³•å¯¹æ¯”ã€‘")
            for method in self.df_summary["method"].unique():
                df_m = self.df_summary[self.df_summary["method"] == method]
                print(f"  {method}:")
                print(f"    RMSE: {df_m['rmse'].mean():.4f}")
                print(f"    è¿è¡Œæ—¶é—´: {df_m['mean_runtime'].mean():.2f}ç§’")

        print("\nã€è´¨é‡è¯„åˆ†åˆ†å¸ƒã€‘(0=æœ€å·®, 4=æœ€å¥½ï¼›æŒ‰æ–¹æ³•æ±‡æ€»)")
        for method, df_m in self.df_bench.groupby("method"):
            qual_dist = df_m["quality_score"].value_counts().sort_index()
            bars = []
            for score, count in qual_dist.items():
                pct = count / len(df_m) * 100
                bars.append(f"{int(score)}åˆ†:{count:3d}({pct:5.1f}%)")
            bars_str = " | ".join(bars)
            print(f"  {method}: {bars_str}")

        iv_rel = self._get(self.df_bench, "iv_relevance_abs_corr")
        excl = self._get(self.df_bench, "iv_exclusion_abs_corr_resid")
        overlap = self._get(self.df_bench, "dr_overlap_score")

        print("\nã€å…³é”®é—®é¢˜æ£€æµ‹ã€‘")
        total = len(self.df_bench)
        weak_iv = (iv_rel < 0.15).sum()
        excl_viol = (excl > 0.2).sum()
        poor_overlap = (overlap < 0.7).sum()
        print(f"  âš ï¸  å¼±IV: {weak_iv}/{total} ({weak_iv/total*100:.1f}%)")
        print(f"  âš ï¸  æ’ä»–æ€§è¿å: {excl_viol}/{total} ({excl_viol/total*100:.1f}%)")
        print(f"  âš ï¸  å·®é‡å : {poor_overlap}/{total} ({poor_overlap/total*100:.1f}%)")
        print("=" * 80)

    # ---------------- identifiability ----------------
    def identifiability_analysis(self) -> None:
        print("\n" + "=" * 80)
        print("1ï¸âƒ£  å¯è¯†åˆ«æ€§åˆ†æ")
        print("=" * 80)

        iv_rel = self._get(self.df_bench, "iv_relevance_abs_corr")
        print("\nã€IVç›¸å…³æ€§ç»Ÿè®¡ã€‘")
        print(f"  å‡å€¼: {iv_rel.mean():.4f}")
        print(f"  ä¸­ä½æ•°: {iv_rel.median():.4f}")
        print(f"  æ ‡å‡†å·®: {iv_rel.std():.4f}")
        print(f"  èŒƒå›´: [{iv_rel.min():.4f}, {iv_rel.max():.4f}]")

        strong = (iv_rel > 0.3).sum()
        moderate = ((iv_rel >= 0.15) & (iv_rel <= 0.3)).sum()
        weak = (iv_rel < 0.15).sum()

        print("\n  å¼ºåº¦åˆ†ç±»:")
        print(f"    å¼º (>0.3):     {strong:3d} ({strong/len(iv_rel)*100:5.1f}%)")
        print(f"    ä¸­ç­‰ (0.15-0.3): {moderate:3d} ({moderate/len(iv_rel)*100:5.1f}%)")
        print(f"    å¼± (<0.15):    {weak:3d} ({weak/len(iv_rel)*100:5.1f}%) âš ï¸")

        mask = iv_rel.notna()
        corr_iv_err = np.corrcoef(
            iv_rel[mask],
            self.df_bench.loc[mask, "abs_err"],
        )[0, 1]
        print(f"\n  ğŸ“Š IVå¼ºåº¦ä¸è¯¯å·®çš„ç›¸å…³æ€§: {corr_iv_err:.4f}")
        if corr_iv_err < -0.2:
            print("     âœ“ å¼ºIVæ˜¾è‘—å‡å°‘è¯¯å·®")
        elif corr_iv_err < -0.1:
            print("     â†’ IVå¼ºåº¦æœ‰åŠ©äºå‡å°‘è¯¯å·®")
        else:
            print("     âš ï¸ IVå¼ºåº¦ä¸è¯¯å·®å…³ç³»ä¸æ˜æ˜¾")

        iv_exc = self._get(self.df_bench, "iv_exclusion_abs_corr_resid")
        print("\nã€æ’ä»–æ€§çº¦æŸæ£€æŸ¥ã€‘")
        print(f"  å‡å€¼: {iv_exc.mean():.4f}")
        print(f"  ä¸­ä½æ•°: {iv_exc.median():.4f}")

        good_excl = (iv_exc < 0.1).sum()
        quest_excl = ((iv_exc >= 0.1) & (iv_exc < 0.2)).sum()
        viol_excl = (iv_exc >= 0.2).sum()

        print("\n  è¿åç¨‹åº¦:")
        print(f"    è‰¯å¥½ (<0.1):   {good_excl:3d} ({good_excl/len(iv_exc)*100:5.1f}%) âœ“")
        print(f"    å¯ç–‘ (0.1-0.2): {quest_excl:3d} ({quest_excl/len(iv_exc)*100:5.1f}%)")
        print(f"    è¿å (>0.2):   {viol_excl:3d} ({viol_excl/len(iv_exc)*100:5.1f}%) âš ï¸")

    # ---------------- representation ----------------
    def representation_quality_analysis(self) -> None:
        print("\n" + "=" * 80)
        print("2ï¸âƒ£  è¡¨å¾è´¨é‡åˆ†æ")
        print("=" * 80)

        if "method" not in self.df_bench.columns:
            # Fallback to overall statistics when method column is absent
            groups = [("ALL", self.df_bench)]
        else:
            groups = list(self.df_bench.groupby("method"))

        for method, df_m in groups:
            print(f"\n--- æ–¹æ³•: {method} ---")

            z_auc = self._get(df_m, "rep_auc_z_to_a")
            print("ã€Zâ†’Aé¢„æµ‹ï¼ˆIVå¼ºåº¦ï¼‰ã€‘")
            print(f"  å‡å€¼: {z_auc.mean():.4f}")
            print(f"  ä¸­ä½æ•°: {z_auc.median():.4f}")
            strong_z = (z_auc > 0.7).sum()
            mod_z = ((z_auc >= 0.6) & (z_auc <= 0.7)).sum()
            weak_z = (z_auc < 0.6).sum()
            print(f"  å¼º (>0.7):   {strong_z:3d} ({strong_z/len(z_auc)*100:5.1f}%) âœ“")
            print(f"  ä¸­ç­‰ (0.6-0.7): {mod_z:3d} ({mod_z/len(z_auc)*100:5.1f}%)")
            print(f"  å¼± (<0.6):   {weak_z:3d} ({weak_z/len(z_auc)*100:5.1f}%) âš ï¸")

            w_auc = self._get(df_m, "rep_auc_w_to_a", 0.5)
            print("ã€Wâ†’Aé¢„æµ‹ï¼ˆåº”è¯¥~0.5ï¼‰ã€‘")
            print(f"  å‡å€¼: {w_auc.mean():.4f}")
            print(f"  ä¸­ä½æ•°: {w_auc.median():.4f}")
            indep_w = ((w_auc > 0.45) & (w_auc < 0.55)).sum()
            dep_w = ((w_auc <= 0.45) | (w_auc >= 0.55)).sum()
            print(f"  ç‹¬ç«‹ (0.45-0.55): {indep_w:3d} ({indep_w/len(w_auc)*100:5.1f}%) âœ“")
            print(f"  ä¾èµ– (å…¶ä»–):    {dep_w:3d} ({dep_w/len(w_auc)*100:5.1f}%) âš ï¸")

            leak = self._get(df_m, "rep_exclusion_leakage_r2", 0.0)
            print("ã€æ’ä»–æ€§æ³„éœ²ï¼ˆZâ†’Yï¼‰ã€‘")
            print(f"  å‡å€¼: {leak.mean():.4f}")
            print(f"  ä¸­ä½æ•°: {leak.median():.4f}")
            good_leak = (leak < 0.1).sum()
            mod_leak = ((leak >= 0.1) & (leak < 0.2)).sum()
            bad_leak = (leak >= 0.2).sum()
            print(f"  ä½ (<0.1):    {good_leak:3d} ({good_leak/len(leak)*100:5.1f}%) âœ“")
            print(f"  ä¸­ç­‰ (0.1-0.2): {mod_leak:3d} ({mod_leak/len(leak)*100:5.1f}%)")
            print(f"  é«˜ (>0.2):    {bad_leak:3d} ({bad_leak/len(leak)*100:5.1f}%) âš ï¸")

            r2_y = self._get(df_m, "rep_r2_xw_a_to_y", 0.0)
            print("ã€ç»“æœé¢„æµ‹ï¼ˆ[X,W,A]â†’Yï¼‰ã€‘")
            print(f"  å‡å€¼: {r2_y.mean():.4f}")
            print(f"  ä¸­ä½æ•°: {r2_y.median():.4f}")
            good_r2 = (r2_y > 0.3).sum()
            poor_r2 = (r2_y <= 0.3).sum()
            print(f"  å¥½ (>0.3): {good_r2:3d} ({good_r2/len(r2_y)*100:5.1f}%) âœ“")
            print(f"  å·® (â‰¤0.3): {poor_r2:3d} ({poor_r2/len(r2_y)*100:5.1f}%) âš ï¸")

    # ---------------- propensity / overlap ----------------
    def propensity_overlap_analysis(self) -> None:
        print("\n" + "=" * 80)
        print("3ï¸âƒ£  å€¾å‘æ€§å¾—åˆ†ä¸é‡å æ€§åˆ†æ")
        print("=" * 80)

        print("\nã€å€¾å‘æ€§å¾—åˆ†èŒƒå›´ã€‘")
        e_min = self._get(self.df_bench, "dr_e_min")
        e_max = self._get(self.df_bench, "dr_e_max")
        print(f"  æœ€å°å€¼å‡å€¼: {e_min.mean():.4f}")
        print(f"  æœ€å¤§å€¼å‡å€¼: {e_max.mean():.4f}")

        extreme_low = (e_min < 0.01).sum()
        extreme_high = (e_max > 0.99).sum()
        if extreme_low > 0:
            print(f"\n  âš ï¸ {extreme_low}ä¸ªè¿è¡Œæœ‰æä½å€¾å‘æ€§å¾—åˆ† (<0.01)")
        if extreme_high > 0:
            print(f"  âš ï¸ {extreme_high}ä¸ªè¿è¡Œæœ‰æé«˜å€¾å‘æ€§å¾—åˆ† (>0.99)")

        print("\nã€æˆªæ–­ç»Ÿè®¡ã€‘")
        clip_used = self._get(self.df_bench, "dr_clip_used").mean()
        frac_clipped = self._get(self.df_bench, "dr_frac_e_clipped").mean()
        print(f"  ä½¿ç”¨çš„æˆªæ–­é˜ˆå€¼: {clip_used:.4f}")
        print(f"  è¢«æˆªæ–­çš„å¹³å‡æ¯”ä¾‹: {frac_clipped*100:.1f}%")
        if frac_clipped > 0.25:
            print("  âš ï¸ è­¦å‘Šï¼šè¶…è¿‡25%çš„è§‚æµ‹è¢«æˆªæ–­ï¼Œå¯èƒ½è¿‡äºæ¿€è¿›")
        elif frac_clipped > 0.15:
            print("  âš ï¸ æ³¨æ„ï¼šæˆªæ–­æ¯”ä¾‹è¾ƒé«˜")
        else:
            print("  âœ“ æˆªæ–­æ¯”ä¾‹åˆç†")

        print("\nã€é‡å æ€§å¾—åˆ†ã€‘")
        overlap = self._get(self.df_bench, "dr_overlap_score")
        print(f"  å‡å€¼: {overlap.mean():.4f}")
        print(f"  ä¸­ä½æ•°: {overlap.median():.4f}")
        print(f"  èŒƒå›´: [{overlap.min():.4f}, {overlap.max():.4f}]")
        good_overlap = (overlap > 0.7).sum()
        mod_overlap = ((overlap >= 0.5) & (overlap <= 0.7)).sum()
        poor_overlap = (overlap < 0.5).sum()
        print(f"\n  å¥½ (>0.7):   {good_overlap:3d} ({good_overlap/len(overlap)*100:5.1f}%) âœ“")
        print(f"  ä¸­ç­‰ (0.5-0.7): {mod_overlap:3d} ({mod_overlap/len(overlap)*100:5.1f}%)")
        print(f"  å·® (<0.5):   {poor_overlap:3d} ({poor_overlap/len(overlap)*100:5.1f}%) âš ï¸")

        if "dr_ess_raw" in self.df_bench.columns:
            ess = self.df_bench["dr_ess_raw"]
            n_total = len(self.df_bench)
            ess_ratio = ess / n_total
            print("\nã€æœ‰æ•ˆæ ·æœ¬é‡(ESS)ã€‘")
            print(f"  ESSå‡å€¼: {ess.mean():.1f}")
            print(f"  ESS/næ¯”ä¾‹: {ess_ratio.mean():.1%}")
            if ess_ratio.mean() < 0.8:
                print("  âš ï¸ ESSæ¯”ä¾‹åä½ï¼Œæƒé‡æ–¹å·®è¾ƒå¤§")

    # ---------------- weights ----------------
    def weight_analysis(self) -> None:
        print("\n" + "=" * 80)
        print("4ï¸âƒ£  IPWæƒé‡åˆ†æ")
        print("=" * 80)

        max_raw = self._get(self.df_bench, "dr_ipw_abs_max_raw")
        print("\nã€åŸå§‹æƒé‡ï¼ˆæˆªæ–­å‰ï¼‰ã€‘")
        print(f"  æœ€å¤§ç»å¯¹å€¼å‡å€¼: {max_raw.mean():.2f}")
        print(f"  æœ€å¤§ç»å¯¹å€¼ä¸­ä½æ•°: {max_raw.median():.2f}")
        print(f"  æœ€å¤§ç»å¯¹å€¼æœ€å¤§å€¼: {max_raw.max():.2f}")
        extreme = (max_raw > 100).sum()
        if extreme > 0:
            print(f"\n  âš ï¸ {extreme}ä¸ªè¿è¡Œæœ‰æç«¯æƒé‡ (>100)")

        print("\nã€æˆªæ–­åæƒé‡ã€‘")
        max_capped = self._get(self.df_bench, "dr_ipw_abs_max_capped")
        print(f"  æœ€å¤§ç»å¯¹å€¼å‡å€¼: {max_capped.mean():.2f}")
        print(f"  æœ€å¤§ç»å¯¹å€¼ä¸­ä½æ•°: {max_capped.median():.2f}")

        frac_capped = self._get(self.df_bench, "dr_frac_ipw_capped")
        print(f"\nã€æƒé‡æˆªæ–­æ¯”ä¾‹ã€‘")
        print(f"  å‡å€¼: {frac_capped.mean()*100:.1f}%")
        print(f"  ä¸­ä½æ•°: {frac_capped.median()*100:.1f}%")
        high_cap = (frac_capped > 0.2).sum()
        if high_cap > 0:
            print(f"  âš ï¸ {high_cap}ä¸ªè¿è¡Œè¶…è¿‡20%çš„æƒé‡è¢«æˆªæ–­")

        cap_used = self._get(self.df_bench, "dr_cap_used").mean()
        print(f"\nã€æƒé‡ä¸Šé™è®¾ç½®ã€‘")
        print(f"  å¹³å‡ä½¿ç”¨çš„ä¸Šé™: {cap_used:.1f}")

    # ---------------- adversarial ----------------
    def adversarial_training_analysis(self) -> None:
        print("\n" + "=" * 80)
        print("5ï¸âƒ£  å¯¹æŠ—è®­ç»ƒæ•ˆæœåˆ†æ")
        print("=" * 80)

        adv_w = self._get(self.df_bench, "adv_w_acc", 0.5)
        print("\nã€Wå¯¹æŠ—å™¨ï¼ˆåº”è¯¥~0.5ï¼‰ã€‘")
        print(f"  å‡å€¼: {adv_w.mean():.4f}")
        print(f"  ä¸­ä½æ•°: {adv_w.median():.4f}")
        print(f"  æ ‡å‡†å·®: {adv_w.std():.4f}")
        good_w = ((adv_w > 0.45) & (adv_w < 0.55)).sum()
        print(f"  ç‹¬ç«‹æ€§å¥½ (0.45-0.55): {good_w}/{len(adv_w)} ({good_w/len(adv_w)*100:.1f}%)")
        if abs(adv_w.mean() - 0.5) > 0.05:
            print("  âš ï¸ Wå¯¹æŠ—å™¨åç¦»0.5è¾ƒå¤šï¼ŒWå¯èƒ½æœªå®Œå…¨ç‹¬ç«‹äºA")
        else:
            print("  âœ“ Wå¯¹æŠ—å™¨è¡¨ç°è‰¯å¥½ï¼ŒWåŸºæœ¬ç‹¬ç«‹äºA")

        adv_n = self._get(self.df_bench, "adv_n_acc", 0.5)
        print("\nã€Nå¯¹æŠ—å™¨ï¼ˆå™ªå£°ï¼Œåº”è¯¥~0.5ï¼‰ã€‘")
        print(f"  å‡å€¼: {adv_n.mean():.4f}")
        print(f"  æ ‡å‡†å·®: {adv_n.std():.4f}")

        adv_z = self._get(self.df_bench, "adv_z_r2", 0.0)
        print("\nã€Zå¯¹æŠ—å™¨ï¼ˆæ’ä»–æ€§ï¼Œåº”è¯¥~0ï¼‰ã€‘")
        print(f"  å‡å€¼: {adv_z.mean():.4f}")
        print(f"  ä¸­ä½æ•°: {adv_z.median():.4f}")
        good_z = (adv_z < 0.1).sum()
        print(f"  æ’ä»–æ€§å¥½ (RÂ²<0.1): {good_z}/{len(adv_z)} ({good_z/len(adv_z)*100:.1f}%)")
        if adv_z.mean() > 0.1:
            print("  âš ï¸ Zå¯¹Yæœ‰è¾ƒå¼ºé¢„æµ‹èƒ½åŠ›ï¼Œå¯èƒ½è¿åæ’ä»–æ€§")
        else:
            print("  âœ“ Zå¯¹æŠ—å™¨è¡¨ç°è‰¯å¥½ï¼ŒZåŸºæœ¬ä¸é¢„æµ‹Y")

    # ---------------- scenario comparison ----------------
    def scenario_comparison(self) -> None:
        print("\n" + "=" * 80)
        print("6ï¸âƒ£  åœºæ™¯å¯¹æ¯”åˆ†æ")
        print("=" * 80)

        if "scenario" not in self.df_bench.columns:
            print("  âš ï¸ æ•°æ®ä¸­æ²¡æœ‰åœºæ™¯ä¿¡æ¯")
            return

        scenarios = self.df_bench["scenario"].unique()
        print(f"\nå…±æœ‰ {len(scenarios)} ä¸ªåœºæ™¯")
        print("\nåœºæ™¯æ€§èƒ½å¯¹æ¯”ï¼š")
        print("  åœºæ™¯åç§°".ljust(30) + "  RMSE   è¯¯å·®   IVå¼ºåº¦  é‡å ")
        print("  " + "-" * 60)

        for scenario in sorted(scenarios):
            df_s = self.df_bench[self.df_bench["scenario"] == scenario]
            rmse = np.sqrt(df_s["sq_err"].mean())
            mae = df_s["abs_err"].mean()
            iv_rel = self._get(df_s, "iv_relevance_abs_corr").mean()
            overlap = self._get(df_s, "dr_overlap_score").mean()
            print(f"  {scenario[:28].ljust(30)} {rmse:6.3f} {mae:6.3f} {iv_rel:6.3f} {overlap:6.3f}")

        print("\nã€é—®é¢˜åœºæ™¯è¯†åˆ«ã€‘")
        stats: List[Dict[str, float]] = []
        for scenario in scenarios:
            df_s = self.df_bench[self.df_bench["scenario"] == scenario]
            stats.append(
                {
                    "scenario": scenario,
                    "rmse": np.sqrt(df_s["sq_err"].mean()),
                    "iv_rel": self._get(df_s, "iv_relevance_abs_corr").mean(),
                    "overlap": self._get(df_s, "dr_overlap_score").mean(),
                }
            )
        df_stats = pd.DataFrame(stats)

        worst = df_stats.nlargest(3, "rmse")
        print("\n  RMSEæœ€é«˜çš„åœºæ™¯:")
        for _, row in worst.iterrows():
            print(f"    {row['scenario']}: RMSE={row['rmse']:.4f}")

        weak_iv_scenarios = df_stats[df_stats["iv_rel"] < 0.15]
        if len(weak_iv_scenarios) > 0:
            print("\n  å¼±IVåœºæ™¯:")
            for _, row in weak_iv_scenarios.iterrows():
                print(f"    {row['scenario']}: IV={row['iv_rel']:.4f}")

        poor_overlap_scenarios = df_stats[df_stats["overlap"] < 0.6]
        if len(poor_overlap_scenarios) > 0:
            print("\n  å·®é‡å åœºæ™¯:")
            for _, row in poor_overlap_scenarios.iterrows():
                print(f"    {row['scenario']}: overlap={row['overlap']:.4f}")

    # ---------------- recommendations ----------------
    def generate_recommendations(self) -> None:
        print("\n" + "=" * 80)
        print("ğŸ’¡ æ”¹è¿›å»ºè®®")
        print("=" * 80)

        recommendations: List[Dict[str, str]] = []
        iv_rel_mean = self._get(self.df_bench, "iv_relevance_abs_corr").mean()
        if iv_rel_mean < 0.15:
            recommendations.append(
                {
                    "priority": "ğŸ”´ é«˜",
                    "issue": "æ•´ä½“IVå¼ºåº¦åå¼±",
                    "recommendation": (
                        f"å¹³å‡IVç›¸å…³æ€§={iv_rel_mean:.4f} < 0.15\n"
                        "  â†’ æ·»åŠ Fç»Ÿè®¡é‡æ£€éªŒ\n"
                        "  â†’ åœ¨å¼±IVåœºæ™¯è‡ªåŠ¨è­¦å‘Šç”¨æˆ·\n"
                        "  â†’ è€ƒè™‘æ›´å¼ºçš„instrumentsæˆ–TSLSæ–¹æ³•"
                    ),
                }
            )

        frac_clipped = self._get(self.df_bench, "dr_frac_e_clipped").mean()
        if frac_clipped > 0.2:
            recommendations.append(
                {
                    "priority": "ğŸ”´ é«˜",
                    "issue": "å€¾å‘æ€§å¾—åˆ†æˆªæ–­è¿‡åº¦",
                    "recommendation": (
                        f"å¹³å‡æˆªæ–­æ¯”ä¾‹={frac_clipped*100:.1f}% > 20%\n"
                        "  â†’ æé«˜clip_propæˆ–å®ç°æ•°æ®é©±åŠ¨çš„æœ€ä¼˜æˆªæ–­\n"
                        "  â†’ è€ƒè™‘ä½¿ç”¨é‡å æƒé‡"
                    ),
                }
            )

        w_auc_mean = self._get(self.df_bench, "rep_auc_w_to_a", 0.5).mean()
        if abs(w_auc_mean - 0.5) > 0.05:
            recommendations.append(
                {
                    "priority": "ğŸŸ¡ ä¸­",
                    "issue": "Wæœªå……åˆ†ç‹¬ç«‹äºA",
                    "recommendation": (
                        f"Wâ†’Açš„AUC={w_auc_mean:.4f}ï¼Œåç¦»0.5\n"
                        "  â†’ å¯ç”¨æˆ–åŠ å¼ºHSICæƒ©ç½šï¼ˆlambda_hsicï¼‰\n"
                        "  â†’ å¢åŠ gamma_adv_wå¯¹æŠ—å¼ºåº¦\n"
                        "  â†’ æ£€æŸ¥æ¡ä»¶æ­£äº¤æƒ©ç½šå®ç°"
                    ),
                }
            )

        leak_mean = self._get(self.df_bench, "rep_exclusion_leakage_r2", 0.0).mean()
        if leak_mean > 0.15:
            recommendations.append(
                {
                    "priority": "ğŸŸ¡ ä¸­",
                    "issue": "Zâ†’Yå­˜åœ¨æ³„éœ²",
                    "recommendation": (
                        f"æ’ä»–æ€§æ³„éœ²RÂ²={leak_mean:.4f} > 0.15\n"
                        "  â†’ å¢åŠ gamma_adv_zå¯¹æŠ—å¼ºåº¦\n"
                        "  â†’ æ·»åŠ  Sargan-Hansen æ£€éªŒ\n"
                        "  â†’ è€ƒè™‘ç§»é™¤å¯ç–‘çš„ instruments"
                    ),
                }
            )

        max_w_mean = self._get(self.df_bench, "dr_ipw_abs_max_raw").mean()
        if max_w_mean > 50:
            recommendations.append(
                {
                    "priority": "ğŸŸ¢ ä½",
                    "issue": "IPWæƒé‡è¿‡å¤§",
                    "recommendation": (
                        f"å¹³å‡æœ€å¤§æƒé‡={max_w_mean:.1f} > 50\n"
                        "  â†’ å¯è€ƒè™‘é™ä½ ipw_cap æˆ–ä½¿ç”¨æ›´å¹³æ»‘çš„æˆªæ–­\n"
                        "  â†’ æç«¯æƒ…å†µè€ƒè™‘åŒ¹é…æ–¹æ³•"
                    ),
                }
            )

        if recommendations:
            print("")
            for i, rec in enumerate(recommendations, 1):
                print(f"{i}. {rec['priority']} {rec['issue']}")
                print(f"   {rec['recommendation']}\n")
        else:
            print("\nâœ“ æœªæ£€æµ‹åˆ°é‡å¤§é—®é¢˜ï¼Œç®—æ³•æ•´ä½“è¡¨ç°è‰¯å¥½ï¼\n")

    # ---------------- full pipeline ----------------
    def full_analysis(self) -> None:
        self.executive_summary()
        # æ–¹æ³•çº§å¯¹æ¯”
        MethodComparison(self.df_bench, self.df_summary).comprehensive_method_comparison()
        self.identifiability_analysis()
        self.representation_quality_analysis()
        self.propensity_overlap_analysis()
        self.weight_analysis()
        self.adversarial_training_analysis()
        self.scenario_comparison()
        self.generate_recommendations()
        print("\n" + "=" * 80)
        print(" " * 25 + "åˆ†æå®Œæˆ")
        print("=" * 80)


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Analyze IVAPCI v3.3 (v22) results.")
    parser.add_argument("--benchmark-file", type=str, default="simulation_benchmark_results 22.csv")
    parser.add_argument("--diagnostics-file", type=str, default="simulation_diagnostics_results 22.csv")
    parser.add_argument("--summary-file", type=str, default="simulation_benchmark_summary 22.csv")
    return parser.parse_args()


if __name__ == "__main__":
    args = parse_args()
    analyzer = IVAPCIv22Analyzer(
        benchmark_file=args.benchmark_file,
        diagnostics_file=args.diagnostics_file,
        summary_file=args.summary_file,
    )
    analyzer.full_analysis()
