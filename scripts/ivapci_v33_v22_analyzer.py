"""IVAPCI v3.3 (version 22) result analyzer.

Reads benchmark/diagnostic CSVs and prints summary + quality/overlap analyses.
Defaults match the filenames mentioned in user feedback; override via CLI flags.
"""

from __future__ import annotations

import argparse
import warnings
from typing import Dict, List

import numpy as np
import pandas as pd
import seaborn as sns


warnings.filterwarnings("ignore")
sns.set_style("whitegrid")


class IVAPCIv22Analyzer:
    """Analyze IVAPCI v3.3 version-22 experiment results."""

    def __init__(
        self,
        benchmark_file: str = "simulation_benchmark_results 22.csv",
        diagnostics_file: str = "simulation_diagnostics_results 22.csv",
        summary_file: str = "simulation_benchmark_summary 22.csv",
    ):
        print("ğŸ“‚ åŠ è½½æ•°æ®...")
        self.df_bench = pd.read_csv(benchmark_file)
        self.df_diag = pd.read_csv(diagnostics_file)
        self.df_summary = pd.read_csv(summary_file)

        print(f"   âœ“ Benchmark: {len(self.df_bench)} è¡Œ")
        print(f"   âœ“ Diagnostics: {len(self.df_diag)} è¡Œ")
        print(f"   âœ“ Summary: {len(self.df_summary)} è¡Œ")

        self._compute_quality_scores()

    def _get(self, df: pd.DataFrame, key: str, default: float = np.nan) -> pd.Series:
        return df[key] if key in df.columns else pd.Series([default] * len(df))

    def _compute_quality_scores(self) -> None:
        """Compute a simple 0â€“4 quality score per row."""

        def score_row(row: pd.Series) -> int:
            score = 0
            if row.get("rep_auc_z_to_a", 0) > 0.7:
                score += 1
            w_auc = row.get("rep_auc_w_to_a", 0.5)
            if 0.45 < w_auc < 0.55:
                score += 1
            if row.get("rep_exclusion_leakage_r2", 1.0) < 0.1:
                score += 1
            if row.get("rep_r2_xw_a_to_y", 0) > 0.3:
                score += 1
            return score

        self.df_bench["quality_score"] = self.df_bench.apply(score_row, axis=1)
        self.df_diag["quality_score"] = self.df_diag.apply(score_row, axis=1)

    # ---------------- executive summary ----------------
    def executive_summary(self) -> None:
        print("\n" + "=" * 80)
        print(" " * 20 + "EXECUTIVE SUMMARY")
        print("=" * 80)

        print("\nã€æ•´ä½“æ€§èƒ½ã€‘")
        print(f"  å¹³å‡ç»å¯¹è¯¯å·®: {self.df_bench['abs_err'].mean():.4f}")
        print(f"  RMSE: {np.sqrt(self.df_bench['sq_err'].mean()):.4f}")
        print(f"  å¹³å‡è¿è¡Œæ—¶é—´: {self.df_bench['runtime_sec'].mean():.2f}ç§’")

        if "method" in self.df_summary.columns:
            print("\nã€æ–¹æ³•å¯¹æ¯”ã€‘")
            for method in self.df_summary["method"].unique():
                df_m = self.df_summary[self.df_summary["method"] == method]
                print(f"  {method}:")
                print(f"    RMSE: {df_m['rmse'].mean():.4f}")
                print(f"    è¿è¡Œæ—¶é—´: {df_m['mean_runtime'].mean():.2f}ç§’")

        print("\nã€è´¨é‡è¯„åˆ†åˆ†å¸ƒã€‘(0=æœ€å·®, 4=æœ€å¥½ï¼›æŒ‰æ–¹æ³•æ±‡æ€»)")
        for method, df_m in self.df_bench.groupby("method"):
            qual_dist = df_m["quality_score"].value_counts().sort_index()
            bars = []
            for score, count in qual_dist.items():
                pct = count / len(df_m) * 100
                bars.append(f"{int(score)}åˆ†:{count:3d}({pct:5.1f}%)")
            bars_str = " | ".join(bars)
            print(f"  {method}: {bars_str}")

        iv_rel = self._get(self.df_bench, "iv_relevance_abs_corr")
        excl = self._get(self.df_bench, "iv_exclusion_abs_corr_resid")
        overlap = self._get(self.df_bench, "dr_overlap_score")

        print("\nã€å…³é”®é—®é¢˜æ£€æµ‹ã€‘")
        total = len(self.df_bench)
        weak_iv = (iv_rel < 0.15).sum()
        excl_viol = (excl > 0.2).sum()
        poor_overlap = (overlap < 0.7).sum()
        print(f"  âš ï¸  å¼±IV: {weak_iv}/{total} ({weak_iv/total*100:.1f}%)")
        print(f"  âš ï¸  æ’ä»–æ€§è¿å: {excl_viol}/{total} ({excl_viol/total*100:.1f}%)")
        print(f"  âš ï¸  å·®é‡å : {poor_overlap}/{total} ({poor_overlap/total*100:.1f}%)")
        print("=" * 80)

    # ---------------- identifiability ----------------
    def identifiability_analysis(self) -> None:
        print("\n" + "=" * 80)
        print("1ï¸âƒ£  å¯è¯†åˆ«æ€§åˆ†æ")
        print("=" * 80)

        iv_rel = self._get(self.df_bench, "iv_relevance_abs_corr")
        print("\nã€IVç›¸å…³æ€§ç»Ÿè®¡ã€‘")
        print(f"  å‡å€¼: {iv_rel.mean():.4f}")
        print(f"  ä¸­ä½æ•°: {iv_rel.median():.4f}")
        print(f"  æ ‡å‡†å·®: {iv_rel.std():.4f}")
        print(f"  èŒƒå›´: [{iv_rel.min():.4f}, {iv_rel.max():.4f}]")

        strong = (iv_rel > 0.3).sum()
        moderate = ((iv_rel >= 0.15) & (iv_rel <= 0.3)).sum()
        weak = (iv_rel < 0.15).sum()

        print("\n  å¼ºåº¦åˆ†ç±»:")
        print(f"    å¼º (>0.3):     {strong:3d} ({strong/len(iv_rel)*100:5.1f}%)")
        print(f"    ä¸­ç­‰ (0.15-0.3): {moderate:3d} ({moderate/len(iv_rel)*100:5.1f}%)")
        print(f"    å¼± (<0.15):    {weak:3d} ({weak/len(iv_rel)*100:5.1f}%) âš ï¸")

        mask = iv_rel.notna()
        corr_iv_err = np.corrcoef(
            iv_rel[mask],
            self.df_bench.loc[mask, "abs_err"],
        )[0, 1]
        print(f"\n  ğŸ“Š IVå¼ºåº¦ä¸è¯¯å·®çš„ç›¸å…³æ€§: {corr_iv_err:.4f}")
        if corr_iv_err < -0.2:
            print("     âœ“ å¼ºIVæ˜¾è‘—å‡å°‘è¯¯å·®")
        elif corr_iv_err < -0.1:
            print("     â†’ IVå¼ºåº¦æœ‰åŠ©äºå‡å°‘è¯¯å·®")
        else:
            print("     âš ï¸ IVå¼ºåº¦ä¸è¯¯å·®å…³ç³»ä¸æ˜æ˜¾")

        iv_exc = self._get(self.df_bench, "iv_exclusion_abs_corr_resid")
        print("\nã€æ’ä»–æ€§çº¦æŸæ£€æŸ¥ã€‘")
        print(f"  å‡å€¼: {iv_exc.mean():.4f}")
        print(f"  ä¸­ä½æ•°: {iv_exc.median():.4f}")

        good_excl = (iv_exc < 0.1).sum()
        quest_excl = ((iv_exc >= 0.1) & (iv_exc < 0.2)).sum()
        viol_excl = (iv_exc >= 0.2).sum()

        print("\n  è¿åç¨‹åº¦:")
        print(f"    è‰¯å¥½ (<0.1):   {good_excl:3d} ({good_excl/len(iv_exc)*100:5.1f}%) âœ“")
        print(f"    å¯ç–‘ (0.1-0.2): {quest_excl:3d} ({quest_excl/len(iv_exc)*100:5.1f}%)")
        print(f"    è¿å (>0.2):   {viol_excl:3d} ({viol_excl/len(iv_exc)*100:5.1f}%) âš ï¸")

    # ---------------- representation ----------------
    def representation_quality_analysis(self) -> None:
        print("\n" + "=" * 80)
        print("2ï¸âƒ£  è¡¨å¾è´¨é‡åˆ†æ")
        print("=" * 80)

        if "method" not in self.df_bench.columns:
            # Fallback to overall statistics when method column is absent
            groups = [("ALL", self.df_bench)]
        else:
            groups = list(self.df_bench.groupby("method"))

        for method, df_m in groups:
            print(f"\n--- æ–¹æ³•: {method} ---")

            z_auc = self._get(df_m, "rep_auc_z_to_a")
            print("ã€Zâ†’Aé¢„æµ‹ï¼ˆIVå¼ºåº¦ï¼‰ã€‘")
            print(f"  å‡å€¼: {z_auc.mean():.4f}")
            print(f"  ä¸­ä½æ•°: {z_auc.median():.4f}")
            strong_z = (z_auc > 0.7).sum()
            mod_z = ((z_auc >= 0.6) & (z_auc <= 0.7)).sum()
            weak_z = (z_auc < 0.6).sum()
            print(f"  å¼º (>0.7):   {strong_z:3d} ({strong_z/len(z_auc)*100:5.1f}%) âœ“")
            print(f"  ä¸­ç­‰ (0.6-0.7): {mod_z:3d} ({mod_z/len(z_auc)*100:5.1f}%)")
            print(f"  å¼± (<0.6):   {weak_z:3d} ({weak_z/len(z_auc)*100:5.1f}%) âš ï¸")

            w_auc = self._get(df_m, "rep_auc_w_to_a", 0.5)
            print("ã€Wâ†’Aé¢„æµ‹ï¼ˆåº”è¯¥~0.5ï¼‰ã€‘")
            print(f"  å‡å€¼: {w_auc.mean():.4f}")
            print(f"  ä¸­ä½æ•°: {w_auc.median():.4f}")
            indep_w = ((w_auc > 0.45) & (w_auc < 0.55)).sum()
            dep_w = ((w_auc <= 0.45) | (w_auc >= 0.55)).sum()
            print(f"  ç‹¬ç«‹ (0.45-0.55): {indep_w:3d} ({indep_w/len(w_auc)*100:5.1f}%) âœ“")
            print(f"  ä¾èµ– (å…¶ä»–):    {dep_w:3d} ({dep_w/len(w_auc)*100:5.1f}%) âš ï¸")

            leak = self._get(df_m, "rep_exclusion_leakage_r2", 0.0)
            print("ã€æ’ä»–æ€§æ³„éœ²ï¼ˆZâ†’Yï¼‰ã€‘")
            print(f"  å‡å€¼: {leak.mean():.4f}")
            print(f"  ä¸­ä½æ•°: {leak.median():.4f}")
            good_leak = (leak < 0.1).sum()
            mod_leak = ((leak >= 0.1) & (leak < 0.2)).sum()
            bad_leak = (leak >= 0.2).sum()
            print(f"  ä½ (<0.1):    {good_leak:3d} ({good_leak/len(leak)*100:5.1f}%) âœ“")
            print(f"  ä¸­ç­‰ (0.1-0.2): {mod_leak:3d} ({mod_leak/len(leak)*100:5.1f}%)")
            print(f"  é«˜ (>0.2):    {bad_leak:3d} ({bad_leak/len(leak)*100:5.1f}%) âš ï¸")

            r2_y = self._get(df_m, "rep_r2_xw_a_to_y", 0.0)
            print("ã€ç»“æœé¢„æµ‹ï¼ˆ[X,W,A]â†’Yï¼‰ã€‘")
            print(f"  å‡å€¼: {r2_y.mean():.4f}")
            print(f"  ä¸­ä½æ•°: {r2_y.median():.4f}")
            good_r2 = (r2_y > 0.3).sum()
            poor_r2 = (r2_y <= 0.3).sum()
            print(f"  å¥½ (>0.3): {good_r2:3d} ({good_r2/len(r2_y)*100:5.1f}%) âœ“")
            print(f"  å·® (â‰¤0.3): {poor_r2:3d} ({poor_r2/len(r2_y)*100:5.1f}%) âš ï¸")

    # ---------------- propensity / overlap ----------------
    def propensity_overlap_analysis(self) -> None:
        print("\n" + "=" * 80)
        print("3ï¸âƒ£  å€¾å‘æ€§å¾—åˆ†ä¸é‡å æ€§åˆ†æ")
        print("=" * 80)

        print("\nã€å€¾å‘æ€§å¾—åˆ†èŒƒå›´ã€‘")
        e_min = self._get(self.df_bench, "dr_e_min")
        e_max = self._get(self.df_bench, "dr_e_max")
        print(f"  æœ€å°å€¼å‡å€¼: {e_min.mean():.4f}")
        print(f"  æœ€å¤§å€¼å‡å€¼: {e_max.mean():.4f}")

        extreme_low = (e_min < 0.01).sum()
        extreme_high = (e_max > 0.99).sum()
        if extreme_low > 0:
            print(f"\n  âš ï¸ {extreme_low}ä¸ªè¿è¡Œæœ‰æä½å€¾å‘æ€§å¾—åˆ† (<0.01)")
        if extreme_high > 0:
            print(f"  âš ï¸ {extreme_high}ä¸ªè¿è¡Œæœ‰æé«˜å€¾å‘æ€§å¾—åˆ† (>0.99)")

        print("\nã€æˆªæ–­ç»Ÿè®¡ã€‘")
        clip_used = self._get(self.df_bench, "dr_clip_used").mean()
        frac_clipped = self._get(self.df_bench, "dr_frac_e_clipped").mean()
        print(f"  ä½¿ç”¨çš„æˆªæ–­é˜ˆå€¼: {clip_used:.4f}")
        print(f"  è¢«æˆªæ–­çš„å¹³å‡æ¯”ä¾‹: {frac_clipped*100:.1f}%")
        if frac_clipped > 0.25:
            print("  âš ï¸ è­¦å‘Šï¼šè¶…è¿‡25%çš„è§‚æµ‹è¢«æˆªæ–­ï¼Œå¯èƒ½è¿‡äºæ¿€è¿›")
        elif frac_clipped > 0.15:
            print("  âš ï¸ æ³¨æ„ï¼šæˆªæ–­æ¯”ä¾‹è¾ƒé«˜")
        else:
            print("  âœ“ æˆªæ–­æ¯”ä¾‹åˆç†")

        print("\nã€é‡å æ€§å¾—åˆ†ã€‘")
        overlap = self._get(self.df_bench, "dr_overlap_score")
        print(f"  å‡å€¼: {overlap.mean():.4f}")
        print(f"  ä¸­ä½æ•°: {overlap.median():.4f}")
        print(f"  èŒƒå›´: [{overlap.min():.4f}, {overlap.max():.4f}]")
        good_overlap = (overlap > 0.7).sum()
        mod_overlap = ((overlap >= 0.5) & (overlap <= 0.7)).sum()
        poor_overlap = (overlap < 0.5).sum()
        print(f"\n  å¥½ (>0.7):   {good_overlap:3d} ({good_overlap/len(overlap)*100:5.1f}%) âœ“")
        print(f"  ä¸­ç­‰ (0.5-0.7): {mod_overlap:3d} ({mod_overlap/len(overlap)*100:5.1f}%)")
        print(f"  å·® (<0.5):   {poor_overlap:3d} ({poor_overlap/len(overlap)*100:5.1f}%) âš ï¸")

        if "dr_ess_raw" in self.df_bench.columns:
            ess = self.df_bench["dr_ess_raw"]
            n_total = len(self.df_bench)
            ess_ratio = ess / n_total
            print("\nã€æœ‰æ•ˆæ ·æœ¬é‡(ESS)ã€‘")
            print(f"  ESSå‡å€¼: {ess.mean():.1f}")
            print(f"  ESS/næ¯”ä¾‹: {ess_ratio.mean():.1%}")
            if ess_ratio.mean() < 0.8:
                print("  âš ï¸ ESSæ¯”ä¾‹åä½ï¼Œæƒé‡æ–¹å·®è¾ƒå¤§")

    # ---------------- weights ----------------
    def weight_analysis(self) -> None:
        print("\n" + "=" * 80)
        print("4ï¸âƒ£  IPWæƒé‡åˆ†æ")
        print("=" * 80)

        max_raw = self._get(self.df_bench, "dr_ipw_abs_max_raw")
        print("\nã€åŸå§‹æƒé‡ï¼ˆæˆªæ–­å‰ï¼‰ã€‘")
        print(f"  æœ€å¤§ç»å¯¹å€¼å‡å€¼: {max_raw.mean():.2f}")
        print(f"  æœ€å¤§ç»å¯¹å€¼ä¸­ä½æ•°: {max_raw.median():.2f}")
        print(f"  æœ€å¤§ç»å¯¹å€¼æœ€å¤§å€¼: {max_raw.max():.2f}")
        extreme = (max_raw > 100).sum()
        if extreme > 0:
            print(f"\n  âš ï¸ {extreme}ä¸ªè¿è¡Œæœ‰æç«¯æƒé‡ (>100)")

        print("\nã€æˆªæ–­åæƒé‡ã€‘")
        max_capped = self._get(self.df_bench, "dr_ipw_abs_max_capped")
        print(f"  æœ€å¤§ç»å¯¹å€¼å‡å€¼: {max_capped.mean():.2f}")
        print(f"  æœ€å¤§ç»å¯¹å€¼ä¸­ä½æ•°: {max_capped.median():.2f}")

        frac_capped = self._get(self.df_bench, "dr_frac_ipw_capped")
        print(f"\nã€æƒé‡æˆªæ–­æ¯”ä¾‹ã€‘")
        print(f"  å‡å€¼: {frac_capped.mean()*100:.1f}%")
        print(f"  ä¸­ä½æ•°: {frac_capped.median()*100:.1f}%")
        high_cap = (frac_capped > 0.2).sum()
        if high_cap > 0:
            print(f"  âš ï¸ {high_cap}ä¸ªè¿è¡Œè¶…è¿‡20%çš„æƒé‡è¢«æˆªæ–­")

        cap_used = self._get(self.df_bench, "dr_cap_used").mean()
        print(f"\nã€æƒé‡ä¸Šé™è®¾ç½®ã€‘")
        print(f"  å¹³å‡ä½¿ç”¨çš„ä¸Šé™: {cap_used:.1f}")

    # ---------------- adversarial ----------------
    def adversarial_training_analysis(self) -> None:
        print("\n" + "=" * 80)
        print("5ï¸âƒ£  å¯¹æŠ—è®­ç»ƒæ•ˆæœåˆ†æ")
        print("=" * 80)

        adv_w = self._get(self.df_bench, "adv_w_acc", 0.5)
        print("\nã€Wå¯¹æŠ—å™¨ï¼ˆåº”è¯¥~0.5ï¼‰ã€‘")
        print(f"  å‡å€¼: {adv_w.mean():.4f}")
        print(f"  ä¸­ä½æ•°: {adv_w.median():.4f}")
        print(f"  æ ‡å‡†å·®: {adv_w.std():.4f}")
        good_w = ((adv_w > 0.45) & (adv_w < 0.55)).sum()
        print(f"  ç‹¬ç«‹æ€§å¥½ (0.45-0.55): {good_w}/{len(adv_w)} ({good_w/len(adv_w)*100:.1f}%)")
        if abs(adv_w.mean() - 0.5) > 0.05:
            print("  âš ï¸ Wå¯¹æŠ—å™¨åç¦»0.5è¾ƒå¤šï¼ŒWå¯èƒ½æœªå®Œå…¨ç‹¬ç«‹äºA")
        else:
            print("  âœ“ Wå¯¹æŠ—å™¨è¡¨ç°è‰¯å¥½ï¼ŒWåŸºæœ¬ç‹¬ç«‹äºA")

        adv_n = self._get(self.df_bench, "adv_n_acc", 0.5)
        print("\nã€Nå¯¹æŠ—å™¨ï¼ˆå™ªå£°ï¼Œåº”è¯¥~0.5ï¼‰ã€‘")
        print(f"  å‡å€¼: {adv_n.mean():.4f}")
        print(f"  æ ‡å‡†å·®: {adv_n.std():.4f}")

        adv_z = self._get(self.df_bench, "adv_z_r2", 0.0)
        print("\nã€Zå¯¹æŠ—å™¨ï¼ˆæ’ä»–æ€§ï¼Œåº”è¯¥~0ï¼‰ã€‘")
        print(f"  å‡å€¼: {adv_z.mean():.4f}")
        print(f"  ä¸­ä½æ•°: {adv_z.median():.4f}")
        good_z = (adv_z < 0.1).sum()
        print(f"  æ’ä»–æ€§å¥½ (RÂ²<0.1): {good_z}/{len(adv_z)} ({good_z/len(adv_z)*100:.1f}%)")
        if adv_z.mean() > 0.1:
            print("  âš ï¸ Zå¯¹Yæœ‰è¾ƒå¼ºé¢„æµ‹èƒ½åŠ›ï¼Œå¯èƒ½è¿åæ’ä»–æ€§")
        else:
            print("  âœ“ Zå¯¹æŠ—å™¨è¡¨ç°è‰¯å¥½ï¼ŒZåŸºæœ¬ä¸é¢„æµ‹Y")

    # ---------------- scenario comparison ----------------
    def scenario_comparison(self) -> None:
        print("\n" + "=" * 80)
        print("6ï¸âƒ£  åœºæ™¯å¯¹æ¯”åˆ†æ")
        print("=" * 80)

        if "scenario" not in self.df_bench.columns:
            print("  âš ï¸ æ•°æ®ä¸­æ²¡æœ‰åœºæ™¯ä¿¡æ¯")
            return

        scenarios = self.df_bench["scenario"].unique()
        print(f"\nå…±æœ‰ {len(scenarios)} ä¸ªåœºæ™¯")
        print("\nåœºæ™¯æ€§èƒ½å¯¹æ¯”ï¼š")
        print("  åœºæ™¯åç§°".ljust(30) + "  RMSE   è¯¯å·®   IVå¼ºåº¦  é‡å ")
        print("  " + "-" * 60)

        for scenario in sorted(scenarios):
            df_s = self.df_bench[self.df_bench["scenario"] == scenario]
            rmse = np.sqrt(df_s["sq_err"].mean())
            mae = df_s["abs_err"].mean()
            iv_rel = self._get(df_s, "iv_relevance_abs_corr").mean()
            overlap = self._get(df_s, "dr_overlap_score").mean()
            print(f"  {scenario[:28].ljust(30)} {rmse:6.3f} {mae:6.3f} {iv_rel:6.3f} {overlap:6.3f}")

        print("\nã€é—®é¢˜åœºæ™¯è¯†åˆ«ã€‘")
        stats: List[Dict[str, float]] = []
        for scenario in scenarios:
            df_s = self.df_bench[self.df_bench["scenario"] == scenario]
            stats.append(
                {
                    "scenario": scenario,
                    "rmse": np.sqrt(df_s["sq_err"].mean()),
                    "iv_rel": self._get(df_s, "iv_relevance_abs_corr").mean(),
                    "overlap": self._get(df_s, "dr_overlap_score").mean(),
                }
            )
        df_stats = pd.DataFrame(stats)

        worst = df_stats.nlargest(3, "rmse")
        print("\n  RMSEæœ€é«˜çš„åœºæ™¯:")
        for _, row in worst.iterrows():
            print(f"    {row['scenario']}: RMSE={row['rmse']:.4f}")

        weak_iv_scenarios = df_stats[df_stats["iv_rel"] < 0.15]
        if len(weak_iv_scenarios) > 0:
            print("\n  å¼±IVåœºæ™¯:")
            for _, row in weak_iv_scenarios.iterrows():
                print(f"    {row['scenario']}: IV={row['iv_rel']:.4f}")

        poor_overlap_scenarios = df_stats[df_stats["overlap"] < 0.6]
        if len(poor_overlap_scenarios) > 0:
            print("\n  å·®é‡å åœºæ™¯:")
            for _, row in poor_overlap_scenarios.iterrows():
                print(f"    {row['scenario']}: overlap={row['overlap']:.4f}")

    # ---------------- recommendations ----------------
    def generate_recommendations(self) -> None:
        print("\n" + "=" * 80)
        print("ğŸ’¡ æ”¹è¿›å»ºè®®")
        print("=" * 80)

        recommendations: List[Dict[str, str]] = []
        iv_rel_mean = self._get(self.df_bench, "iv_relevance_abs_corr").mean()
        if iv_rel_mean < 0.15:
            recommendations.append(
                {
                    "priority": "ğŸ”´ é«˜",
                    "issue": "æ•´ä½“IVå¼ºåº¦åå¼±",
                    "recommendation": (
                        f"å¹³å‡IVç›¸å…³æ€§={iv_rel_mean:.4f} < 0.15\n"
                        "  â†’ æ·»åŠ Fç»Ÿè®¡é‡æ£€éªŒ\n"
                        "  â†’ åœ¨å¼±IVåœºæ™¯è‡ªåŠ¨è­¦å‘Šç”¨æˆ·\n"
                        "  â†’ è€ƒè™‘æ›´å¼ºçš„instrumentsæˆ–TSLSæ–¹æ³•"
                    ),
                }
            )

        frac_clipped = self._get(self.df_bench, "dr_frac_e_clipped").mean()
        if frac_clipped > 0.2:
            recommendations.append(
                {
                    "priority": "ğŸ”´ é«˜",
                    "issue": "å€¾å‘æ€§å¾—åˆ†æˆªæ–­è¿‡åº¦",
                    "recommendation": (
                        f"å¹³å‡æˆªæ–­æ¯”ä¾‹={frac_clipped*100:.1f}% > 20%\n"
                        "  â†’ æé«˜clip_propæˆ–å®ç°æ•°æ®é©±åŠ¨çš„æœ€ä¼˜æˆªæ–­\n"
                        "  â†’ è€ƒè™‘ä½¿ç”¨é‡å æƒé‡"
                    ),
                }
            )

        w_auc_mean = self._get(self.df_bench, "rep_auc_w_to_a", 0.5).mean()
        if abs(w_auc_mean - 0.5) > 0.05:
            recommendations.append(
                {
                    "priority": "ğŸŸ¡ ä¸­",
                    "issue": "Wæœªå……åˆ†ç‹¬ç«‹äºA",
                    "recommendation": (
                        f"Wâ†’Açš„AUC={w_auc_mean:.4f}ï¼Œåç¦»0.5\n"
                        "  â†’ å¯ç”¨æˆ–åŠ å¼ºHSICæƒ©ç½šï¼ˆlambda_hsicï¼‰\n"
                        "  â†’ å¢åŠ gamma_adv_wå¯¹æŠ—å¼ºåº¦\n"
                        "  â†’ æ£€æŸ¥æ¡ä»¶æ­£äº¤æƒ©ç½šå®ç°"
                    ),
                }
            )

        leak_mean = self._get(self.df_bench, "rep_exclusion_leakage_r2", 0.0).mean()
        if leak_mean > 0.15:
            recommendations.append(
                {
                    "priority": "ğŸŸ¡ ä¸­",
                    "issue": "Zâ†’Yå­˜åœ¨æ³„éœ²",
                    "recommendation": (
                        f"æ’ä»–æ€§æ³„éœ²RÂ²={leak_mean:.4f} > 0.15\n"
                        "  â†’ å¢åŠ gamma_adv_zå¯¹æŠ—å¼ºåº¦\n"
                        "  â†’ æ·»åŠ  Sargan-Hansen æ£€éªŒ\n"
                        "  â†’ è€ƒè™‘ç§»é™¤å¯ç–‘çš„ instruments"
                    ),
                }
            )

        max_w_mean = self._get(self.df_bench, "dr_ipw_abs_max_raw").mean()
        if max_w_mean > 50:
            recommendations.append(
                {
                    "priority": "ğŸŸ¢ ä½",
                    "issue": "IPWæƒé‡è¿‡å¤§",
                    "recommendation": (
                        f"å¹³å‡æœ€å¤§æƒé‡={max_w_mean:.1f} > 50\n"
                        "  â†’ å¯è€ƒè™‘é™ä½ ipw_cap æˆ–ä½¿ç”¨æ›´å¹³æ»‘çš„æˆªæ–­\n"
                        "  â†’ æç«¯æƒ…å†µè€ƒè™‘åŒ¹é…æ–¹æ³•"
                    ),
                }
            )

        if recommendations:
            print("")
            for i, rec in enumerate(recommendations, 1):
                print(f"{i}. {rec['priority']} {rec['issue']}")
                print(f"   {rec['recommendation']}\n")
        else:
            print("\nâœ“ æœªæ£€æµ‹åˆ°é‡å¤§é—®é¢˜ï¼Œç®—æ³•æ•´ä½“è¡¨ç°è‰¯å¥½ï¼\n")

    # ---------------- full pipeline ----------------
    def full_analysis(self) -> None:
        self.executive_summary()
        self.identifiability_analysis()
        self.representation_quality_analysis()
        self.propensity_overlap_analysis()
        self.weight_analysis()
        self.adversarial_training_analysis()
        self.scenario_comparison()
        self.generate_recommendations()
        print("\n" + "=" * 80)
        print(" " * 25 + "åˆ†æå®Œæˆ")
        print("=" * 80)


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Analyze IVAPCI v3.3 (v22) results.")
    parser.add_argument("--benchmark-file", type=str, default="simulation_benchmark_results 22.csv")
    parser.add_argument("--diagnostics-file", type=str, default="simulation_diagnostics_results 22.csv")
    parser.add_argument("--summary-file", type=str, default="simulation_benchmark_summary 22.csv")
    return parser.parse_args()


if __name__ == "__main__":
    args = parse_args()
    analyzer = IVAPCIv22Analyzer(
        benchmark_file=args.benchmark_file,
        diagnostics_file=args.diagnostics_file,
        summary_file=args.summary_file,
    )
    analyzer.full_analysis()
